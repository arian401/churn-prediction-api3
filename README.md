---
title: Customer Churn Prediction API
emoji: üîÅ
colorFrom: blue
colorTo: green
sdk: docker
app_port: 7860
pinned: false
---


# Customer Churn Prediction API ‚Äì Retail Analytics with Machine Learning

This project demonstrates how to build, deploy, and automate a **churn prediction system** for a retail or e-commerce platform using **machine learning** and **FastAPI**.

## Overview

To implement automated **retail analytics with machine learning**, the process begins by training predictive models‚Äîsuch as churn prediction, price sensitivity modeling, or customer segmentation‚Äîusing business-specific data like customer activity and purchase history. This is typically done in a Python script or Jupyter notebook where the model is trained, evaluated, and saved as a `.pkl` file along with any preprocessing tools like scalers.

In this example, customer churn prediction is based on data extracted from a PostgreSQL e-commerce database using SQLAlchemy. The model is trained and exposed via a FastAPI endpoint for real-time prediction integration with the business backend.

---

## ‚öôÔ∏è Features

- ‚úÖ Connect to a live PostgreSQL database
- ‚úÖ Load and preprocess customer records
- ‚úÖ Train a classification model (Random Forest)
- ‚úÖ Export model and scaler as `.pkl` files
- ‚úÖ Serve predictions through a FastAPI POST endpoint
- ‚úÖ Supports JSON input for integration into dashboards or web backends

---

## üóÇÔ∏è Project Structure

```
.
‚îú‚îÄ‚îÄ main.py                  # FastAPI app
‚îú‚îÄ‚îÄ train_and_export.py      # Training + export script
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ churn_model.pkl      # Trained model
‚îÇ   ‚îî‚îÄ‚îÄ scaler.pkl           # Scaler used during preprocessing
‚îú‚îÄ‚îÄ requirements.txt         # Required Python packages
‚îî‚îÄ‚îÄ README.md                # Project documentation
```

---

## Run Locally

### 1. Install dependencies
```bash
pip install -r requirements.txt
```

### 2. Train the model and generate `.pkl` files
```bash
python train_and_export.py
```

> This script connects to a PostgreSQL database, loads customer data using SQLAlchemy, trains a churn prediction model, and exports it.

### 3. Launch the API
```bash
uvicorn main:app --reload
```

### 4. Open in browser (Swagger UI)
```
http://127.0.0.1:8000/docs
```

---

## Example JSON Input

```json
{
  "tenure_months": 12,
  "total_spent": 350.50,
  "avg_purchase_frequency": 3.2,
  "is_active": 1
}
```

---

## üåê Deployment Options

You can host the app and model on:

- **Render.com** (via GitHub auto-deploy)
- **Hugging Face Spaces** (Git-based auto-rebuild)
- **Streamlit** (as UI layer for business teams)

---

## üîÅ Automation Pipeline

To apply the models continuously:

- Pull live customer data from DB or dashboard via API
- Call the model API for predictions
- Trigger business actions based on predictions

For ongoing effectiveness, establish a **weekly retraining pipeline** using:

- `cron` jobs
- **Apache Airflow**
- **GitHub Actions**

The workflow will:
1. Pull fresh data
2. Retrain and generate new `.pkl` files
3. Push to GitHub or Hugging Face ‚Üí auto-redeploy

This end-to-end automation ensures that ML insights stay relevant and actionable without manual intervention.

---

## Contact

For questions, feedback, or collaboration, feel free to open an issue or reach out.
